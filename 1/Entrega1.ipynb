{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "Consider the unconstrained problem\n",
    "$$\n",
    "\\min f(x), \\hspace{0.5 cm} \\text{where  } f(x) = \\left\\{100(x_2-x_1)^2+(1-x_1)^2\\right\\}\n",
    "$$\n",
    "whose exact minimum is at $x^∗ = (1, 1)$. Find an estimate of $x^∗$ using:\n",
    "* [a gradient method](#Gradient-descent),\n",
    "* [a Newton's method](#newtons-method),\n",
    "* [a conjugate direction method](#conjugate-direction-method).\n",
    "\n",
    "In all cases use $x_0=(2,5)$ as starting point, backtracking line search with $\\alpha=0.25$ and $\\beta=0.5$, and $\\|\\nabla f(x_k)\\|<10^{-5}$ as a stopping criterium. How many iterations have you used to find the estimate of $x^∗$ with these methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import ArrayLike\n",
    "import numpy.linalg as npl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "First we define some functions that we are going to use in this homework. These are the function we want to minimize, its gradient and the line search method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([2,5]) # the initial point\n",
    "\n",
    "\n",
    "def f(x: ArrayLike) -> float:\n",
    "    \"\"\"The function that we want to minimize\"\"\"\n",
    "    assert len(x) == 2, \"Size doesn't match, the size must be 2.\"\n",
    "    return 100*(x[1]-x[0])**2 + (1-x[0])**2\n",
    "\n",
    "\n",
    "def gradient(x: ArrayLike) -> ArrayLike:\n",
    "    \"\"\"Returns the value of the gradient at the given point\"\"\"\n",
    "    assert len(x) == 2, \"Size doesn't match, the size must be 2.\"\n",
    "    return np.array([-2*(1-x[0]) - 200*(x[1]-x[0]), 200*(x[1]-x[0])])\n",
    "\n",
    "\n",
    "def line_search(d_k: ArrayLike, x: ArrayLike) -> float:\n",
    "    \"\"\"Implement the line search method, this function update the time step\n",
    "    until the criterion is fulfilled\"\"\"\n",
    "\n",
    "    alpha = 0.25\n",
    "    beta = 0.5\n",
    "    t = 1\n",
    "    while f(x + t*d_k) > f(x) + alpha * t * np.dot(gradient(x),d_k):\n",
    "        t *= beta\n",
    "    return t\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three methods that we have to implement differ only in the way we define our descend direction. The first one is gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this method we set our descent direction $d_k = -\\nabla f(x_k)$.\n",
    "\n",
    "As a stopping criterium we set $\\|\\nabla f(x_k)\\|<10^{-5}$ but have to put another criterium using the number of iterations in case that the other stopping criterium is never achieved. To compute the norm of the gradient we make a dot product. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descend(stop: float, x: ArrayLike) -> tuple[ArrayLike, int]:\n",
    "    \"\"\"The gradient descend method using line search\"\"\"\n",
    "\n",
    "    \n",
    "    grad = gradient(x)\n",
    "    if np.dot(grad, grad) < stop**2: #check the case that we are already in a minimum\n",
    "        return x, 0\n",
    "    \n",
    "    for ite in range(1,10_000):\n",
    "        descend_direction = -grad\n",
    "        t = line_search(descend_direction, x)\n",
    "        x = x + t * descend_direction\n",
    "        grad = gradient(x)\n",
    "        if np.dot(grad, grad) < stop**2: \n",
    "            return x, ite\n",
    "        \n",
    "    print(\"The stop criterion wasn't achieve in 10000 iterations.\")\n",
    "    return x, ite\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimun is achieve at [1.0000059  1.00000595] using 2221 iterations\n"
     ]
    }
   ],
   "source": [
    "mini, iterations = gradient_descend(1.0e-05, X)\n",
    "print(f\"The minimum is achieve at {mini} using {iterations} iterations\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we achieve almost an exact minimum. We compute the method more than 2000 times which is a lot. We can compute the amount of time this method takes to execute and we see that it's around 190 ms which is not a lot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 ms ± 17.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "gradient_descend(1.0e-05, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's method\n",
    "\n",
    "In this case we define the descend direction as $d_k = -(\\nabla^2f(x_k))^{-1}\\nabla f(x_k)$. For this method we need the Hessian matrix $(\\nabla^2f(x_k))$, so we need to compute the derivatives of the functions.\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_1} = -200 (x_2-x_1)-2 (1-x_1), \n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_2} = 200 (x_2-x_1).\n",
    "$$\n",
    "From these expressions we can see that the second derivatives are constant, so the Hessian matrix for every $x_k$ is:\n",
    "$$\n",
    "\\nabla^2f(x)=\\begin{pmatrix} 202 & -200 \\\\ -200 & 200\\end{pmatrix}.\n",
    "$$\n",
    "To compute the descend direction we need the inverse matrix so we are going to compute it:\n",
    "$$\n",
    "(\\nabla^2f(x))^{-1} = \\begin{pmatrix}1/2 & 1/2 \\\\ 1/2 & 101/200 \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "The implementation of this method is the same as the previous one, we only have to change the descend direction. Because computing the inverse of the Hessian (solving a linear system) is part of the method we are going to do it using python, so when we compute the time execution time we take it into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "HESSIAN = np.array([[202, -200], [-200, 200]])\n",
    "\n",
    "def newtons_method(stop: float, x: ArrayLike) -> tuple[ArrayLike, int]:\n",
    "    \"\"\"Implementation of the newton method\"\"\"\n",
    "\n",
    "    grad = gradient(x)\n",
    "    if np.dot(grad, grad) < stop**2:  # check the case that we are already in a minimum\n",
    "        return x, 0\n",
    "\n",
    "    for ite in range(1,10_000):\n",
    "        descend_direction = npl.solve(-HESSIAN, grad)\n",
    "        t = line_search(descend_direction, x)\n",
    "        x = x + t * descend_direction\n",
    "        grad = gradient(x)\n",
    "        if np.dot(grad, grad) < stop**2:\n",
    "            return x, ite\n",
    "\n",
    "    print(\"The stop criterium wasn't achieve in 10000 iterations.\")\n",
    "    return x, ite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimun is achieve at [1. 1.] using 1 iterations\n"
     ]
    }
   ],
   "source": [
    "mini, iterations = newtons_method(1.0e-05, X)\n",
    "print(f\"The minimum is achieve at {mini} using {iterations} iterations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second method is more accurate than the previous one because we get the exact solution. This solution is achieve only after one iteration of the method. The downside of the method is the need to calculate the inverse of a matrix, which we have done solving a system of equations instead of the direct computation of the matrix. This last part may be the reason of such a small time, 40.5 $\\mu s$ on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.4 µs ± 5.39 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "newtons_method(1.0e-05, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate direction method\n",
    "In this case we don't want to compute a matrix inversion. The implementation is the second one we saw in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_direction(stop: float, x: ArrayLike) -> tuple[ArrayLike, int]:\n",
    "    \"\"\"Descend method with conjugate direction\"\"\"\n",
    "\n",
    "    g_k = gradient(x)\n",
    "    if np.dot(g_k, g_k) < stop**2:  # check the case that we are already in a minimum\n",
    "        return x, ite\n",
    "    A = HESSIAN\n",
    "    b = np.dot(A,x) - g_k\n",
    "    descend_direction = -g_k\n",
    "\n",
    "    for ite in range(1,10_000):\n",
    "        alpha = np.dot(g_k, descend_direction)/(npl.multi_dot([descend_direction,A,descend_direction]))\n",
    "        x = x - alpha * descend_direction\n",
    "        g_k = np.dot(A,x) - b\n",
    "        beta = npl.multi_dot([g_k,A,descend_direction])/npl.multi_dot([descend_direction,A,descend_direction])\n",
    "        descend_direction = -g_k + beta * descend_direction\n",
    "\n",
    "        grad = gradient(x)\n",
    "        if np.dot(grad, grad) < stop**2:\n",
    "            return x, ite\n",
    "\n",
    "    print(\"The stop criterium wasn't achieve in 10000 iterations.\")\n",
    "    return x, ite\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimun is achieve at [1. 1.] using 2 iterations\n"
     ]
    }
   ],
   "source": [
    "mini, iterations = conjugate_direction(1.0e-05, X)\n",
    "print(f\"The minimun is achieve at {mini} using {iterations} iterations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this method we also arrive to the exact solution but in this case usign two iterations. Although we didn't solve a system with this method the fact that we need two iterations may be an explanation on the amount of time that this method need, 118 $\\mu s$ on average. Also this problem is a small one and solving a $2\\times 2$ system may be faster than all the matrix multiplications that we need to do in this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 µs ± 5.26 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "conjugate_direction(1.0e-05, X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2acbbc62b63727f51b412b81f44665fa10e0f4ef373d646d67463d6fac66285f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
