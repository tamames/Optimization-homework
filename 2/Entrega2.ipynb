{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "## Exercise 1\n",
    "Assume we have 1 dollar to be invested in two assets, whose return is modeled as a bivariate Gaussian distribution such that\n",
    "\n",
    "$$\n",
    "\\Sigma = \\begin{pmatrix}\n",
    "0.25 & 0.15 \\\\ 0.15 & 0.10\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "How much should we invest in each to minimize the overall variance in return with and without short selling? Answer this question solving the problem analytically and using a numerical code for constrained optimization problems. To simplify the problem, the acceptable baseline expected rate of return $m_b$ is $âˆ’\\infty$.\n",
    "\n",
    "### Analytical solution\n",
    "In this problem we have to minimize the variance with inequality constrains. This variance is computed in the following way $f(\\textbf{w}) = \\textbf{w}^\\top \\Sigma\\textbf{w}$ where each component of the vector $\\textbf{w}$ is the weight that the corresponding asset is going to have in our portfolio.\n",
    "\n",
    "#### With short selling\n",
    "\n",
    "The constrains that we have to take into account are $\\sum_i^nw_i=1$, this means that our weights must sum to 1. Now to solve the problem lets compute the function $f(\\textbf{w})$.\n",
    "\n",
    "$$\n",
    "    f(\\textbf{w}) = \\begin{pmatrix} w_1 & w_2 \\end{pmatrix} \\begin{pmatrix} 0.25 & 0.15 \\\\ 0.15 & 0.10 \\end{pmatrix}\\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}= 0.25 w_1^2+0.3w_1w_2+0.1w_2^2=f(w_1,w_2).\n",
    "$$\n",
    "\n",
    "With this function we have to solve a optimization problem in two dimensions but we can transform it by taking into account the constrain $w_1+w_2=1\\to w_1=1-w_2$. So if we substitute this in the original equation we get $f(w_2) = 0.05w_2^2-0.2w_2+0.25,$ that is a function of only one variable so to find its minimum we just have to derive it.\n",
    "\n",
    "$$\n",
    "f'(w_2) = 0.1w_2-0.2.\n",
    "$$\n",
    "\n",
    "And the minimum is achieved at $0.1w_2 = 0.2\\to w_2 = 2$. With this value of $w_2$ we get $w_1=-1$ and the solution is:\n",
    "\n",
    "$$\n",
    "    \\textbf{w} = \\begin{pmatrix}-1\\\\2\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "With this solution we see that we have to short sell the first asset and then buy the second one.\n",
    "\n",
    "#### Without short selling\n",
    "In this case we don't allow the weights to be negative, but the rest of the problem is the same. We have to implement the following constrains $w_1\\geq0$ and $w_2\\geq0$. Because $w_2$ is positive and $w_1=1-w_2$ is also positive we have that $w_2$ must be also less or equal than 1, so $w_2\\in[0,1]$. The absolute minimum from of the function $f(w_2)$, obtained in [the previous section](#with-short-selling), is not inside this region so because we know that this function only has an extrema point the minimum value inside the interval must be achieved at the boundaries.\n",
    "\n",
    "$$\n",
    "f(0) = 0.25 \\;\\;\\;\\text{and}\\;\\;\\; f(1) = 0.1\n",
    "$$\n",
    "\n",
    "So the minimum is at $w_2=1$ therefore $w_1$ must be 0.\n",
    "\n",
    "$$\n",
    "    \\textbf{w} = \\begin{pmatrix}0\\\\1\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "## Numeric solution\n",
    "To find a numerical solution we are going to implement something similar to the Newton method that we use for the previous assignment. In order for the Newton method to work with equality constrains we have to choose as a starting point a feasible one and we have to redefine the step of the Newton method.\n",
    "\n",
    "First we are going to define some functions that we are going to use, as the function itself, the gradient and the line search method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as npl\n",
    "\n",
    "def f(w):\n",
    "    return 0.25*w[0]**2 + 0.30*w[0]*w[1] + 0.1*w[1]**2\n",
    "\n",
    "def gradient(w):\n",
    "    return np.array([0.5*w[0] + 0.3*w[1], 0.3*w[0] + 0.2*w[1]])\n",
    "\n",
    "def line_search(d_k, x, t=1):\n",
    "    \"\"\"Implement the line search method, this function update the time step\n",
    "    until the criterion is fulfilled\"\"\"\n",
    "\n",
    "    alpha = 0.01\n",
    "    beta = 0.5\n",
    "    while f(x + t*d_k) > f(x) + alpha*t*np.dot(gradient(x),d_k):\n",
    "        t *= beta\n",
    "    return t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous assignment we need to compute the Hessian matrix, in this case this matrix is:\n",
    "\n",
    "$$\n",
    "H = \\begin{pmatrix}\n",
    "\\partial^2_{w_1}f & \\partial_{w_1}\\partial_{w_2}f \\\\\n",
    "\\partial_{w_1}\\partial_{w_2}f & \\partial^2_{w_2}f\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "0.5 & 0.3 \\\\ 0.3 & 0.2\n",
    "\\end{pmatrix} = \\nabla^2f\n",
    "$$\n",
    "\n",
    "We have to redefine the step of this method and to do so we need to solve a different system that the one on the previous assignment. In this case the system that we have to solve is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\nabla^2f & A^\\top \\\\ A & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} d_k \\\\ \\lambda_k \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} -\\nabla f \\\\ 0 \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where $A = \\begin{pmatrix} 1 & 1 \\end{pmatrix}$, is the matrix that gives us the constrain $A\\text{w}=1$.\n",
    "\n",
    "Now lets put the code for the newton method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "HESSIAN = np.array([[0.5, 0.3], [0.3, 0.2]])\n",
    "A = np.array([[1.0, 1.0]])\n",
    "# Define the matrix above\n",
    "MATRIX = np.zeros((3, 3))\n",
    "MATRIX[:2,:2] = HESSIAN\n",
    "MATRIX[:2,-1:] = A.T\n",
    "MATRIX[-1:,:2] = A\n",
    "\n",
    "def newtons_method(stop, x, short_selling = False):\n",
    "    \"\"\"Implementation of the newton method\"\"\"\n",
    "\n",
    "    grad = gradient(x)\n",
    "    if np.dot(grad, grad) < stop**2:  # check the case that we are already in a minimum\n",
    "        return x, 0\n",
    "\n",
    "    for ite in range(1,1000):\n",
    "        g = gradient(x)\n",
    "        lhs = np.concatenate((-g.T,[0])).reshape((3,1))\n",
    "        solution = npl.solve(MATRIX, lhs)\n",
    "        descend_direction = solution[:2].reshape((2,))\n",
    "        \n",
    "        if short_selling:\n",
    "            crit = np.dot(g, descend_direction)\n",
    "            if np.abs(crit) < stop:\n",
    "                return x, ite\n",
    "            t = line_search(descend_direction, x)\n",
    "            x = x + t * descend_direction\n",
    "        else:\n",
    "            t = 1\n",
    "            while min(x + t*descend_direction) < 0:\n",
    "                t *= 0.5\n",
    "            t = line_search(descend_direction, x, t)\n",
    "            x = x + t * descend_direction\n",
    "\n",
    "\n",
    "\n",
    "    print(\"The stop criterium wasn't achieve in 1000 iterations.\")\n",
    "    return x, ite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With short selling\n",
    "In this case the solution *w* could have negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.,  2.]), 2)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newtons_method(1.0e-05, [0.5,0.5], short_selling=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2acbbc62b63727f51b412b81f44665fa10e0f4ef373d646d67463d6fac66285f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
