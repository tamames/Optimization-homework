{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "Consider the unconstrained problem\n",
    "$$\n",
    "\\min f(x), \\hspace{0.5 cm} \\text{where  } f(x) = \\left\\{100(x_2-x_1)^2+(1-x_1)^2\\right\\}\n",
    "$$\n",
    "whose exact minimum is at $x^∗ = (1, 1)$. Find an estimate of $x^∗$ using:\n",
    "* [a gradient method](#Gradient-descent),\n",
    "* a Newton's method,\n",
    "* a conjugate direction method.\n",
    "\n",
    "In all cases use $x_0=(2,5)$ as starting point, backtracking line search with $\\alpha=0.25$ and $\\beta=0.5$, and $\\|\\nabla f(x_k)\\|<10^{-5}$ as a stopping criterium. How many iterations have you used to find the estimate of $x^∗$ with these methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import ArrayLike"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "First we define some common functions like the function we want to minimize, its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([2,5]) # the initial point\n",
    "\n",
    "\n",
    "def f(x: ArrayLike) -> float:\n",
    "    \"\"\"The function that we want to minimize\"\"\"\n",
    "    assert len(x) == 2, \"Size doesn't match, the size must be 2.\"\n",
    "    return 100*(x[1]-x[0])**2 + (1-x[0])**2\n",
    "\n",
    "\n",
    "def gradient(x: ArrayLike) -> ArrayLike:\n",
    "    \"\"\"Returns the value of the gradient at the given point\"\"\"\n",
    "    assert len(x) == 2, \"Size doesn't match, the size must be 2.\"\n",
    "    return np.array([-2*(1-x[0]) - 200*(x[1]-x[0]), 200*(x[1]-x[0])])\n",
    "\n",
    "\n",
    "def line_search(d_k: ArrayLike, x: ArrayLike) -> float:\n",
    "    \"\"\"Implement the line search method, this function update the time step\n",
    "    until the criterion is fulfilled\"\"\"\n",
    "\n",
    "    alpha = 0.25\n",
    "    beta = 0.5\n",
    "    t = 1\n",
    "    while f(x + t*d_k) > f(x) + alpha * t * np.dot(gradient(x),d_k):\n",
    "        t *= beta\n",
    "    return t\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three methods that we have to implement differ only in the way we define our descend direction. The first one is gradient descent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this method we set our descent direction $d_k = -\\nabla f(x_k)$.\n",
    "\n",
    "As a stopping criterium we set $\\|\\nabla f(x_k)\\|<10^{-5}$ but have to put another criterium using the number of iterations in case that the other stopping criterium is never achieved. To compute the norm of the gradient we make a dot product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descend(stop: float, x: ArrayLike) -> tuple[ArrayLike, int]:\n",
    "    \"\"\"The gradient descend method using line search\"\"\"\n",
    "\n",
    "    grad = gradient(x)\n",
    "    \n",
    "    for ite in range(10_000):\n",
    "        descend_direction = -grad\n",
    "        t = line_search(descend_direction, x)\n",
    "        x = x + t * descend_direction\n",
    "        grad = gradient(x)\n",
    "        if np.dot(grad, grad) < stop**2: \n",
    "            return x, ite\n",
    "        \n",
    "    print(\"The stop criterium wasn't achieve in 10000 iterations.\")\n",
    "    return x, ite\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.0000059 , 1.00000595]), 2220)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descend(1.0e-05, X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's method\n",
    "\n",
    "In this case we define the descend direction as $d_k = -(\\nabla^2f(x_k))^{-1}\\nabla f(x_k)$. For this method we need the Hessian matrix $(\\nabla^2f(x_k))$, so we need to compute the derivatives of the functions.\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_1} = -200 (x_2-x_1)-2 (1-x_1), \n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_2} = 200 (x_2-x_1).\n",
    "$$\n",
    "From these expressions we can see that the second derivatives are constant, so the Hessian matrix is:\n",
    "$$\n",
    "\\nabla^2f(x)=\\begin{pmatrix} 202 & -200 \\\\ -200 & 200\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "HESSIAN = np.array([[202, -200], [-200, 200]])\n",
    "\n",
    "\n",
    "def newtons_method(stop: float, x: ArrayLike) -> tuple[ArrayLike, int]:\n",
    "    pass \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2acbbc62b63727f51b412b81f44665fa10e0f4ef373d646d67463d6fac66285f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
